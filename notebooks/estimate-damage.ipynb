{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a27a3b",
   "metadata": {},
   "source": [
    "# Flood Damage Estimates to Roads.\n",
    "\n",
    "The purpose of this jupyter notebook is to calculate damage inflicted on roads and give a representation of the results.\n",
    "\n",
    "The damage is measured in damage-meter. The uncertainty of the inflicted damage stems from both the uncertainty with respect to the intensity ($I$) of the flood and the uncertainty associated with the assigned damage under the given intensity ($F$). To enable the quantification of uncertainty with respect to the fragility (or assigned damage) the conditional expectation of the damage of segment $i$ ($D_i$) is estimated. That is:\n",
    "$$\n",
    "    E[D_i|F]\n",
    "$$\n",
    "The purpose is to quantify uncertainty associated with the damage mapping (as) separate from the uncertainty with respect to the flood scenarios. Fragility is sampled using a Gausian Field with an exponential kernel. The length scale (decorrelation length) has been set to 1 km.\n",
    "\n",
    "The framework is elaborated in methods description.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "There are a couple of preprocessing steps necesarry before running this notebook. Most of these are automated through a series of python scripts. Proceedure is described in `README.md`. Each script is implemented with a command line interface. Use the help function for further documentation.\n",
    "\n",
    "NB: Recall to set parameters in `config.py`. The preprocessing scripts only apply the logging settings.\n",
    "\n",
    "Before running this notebook, fit the damage function using the notebook `damage-function.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919cfea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "from rasterio.windows import Window\n",
    "from pyproj import Proj, CRS, Transformer\n",
    "\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24928536",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/home/erlend/data/portugal-rerun\"\n",
    "SRCDIR = \"/home/erlend/projects/gp-damage-aggregator\"\n",
    "os.chdir(DATADIR)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable DATADIR\n",
    "%env DATADIR $DATADIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97348674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37656cbd",
   "metadata": {},
   "source": [
    "This is the folder after preprocessing steps are finished. `src` folder is a link to the folder containing the scripts. The `random_fields` folder contains one subfolder for each set of random field files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls random_fields/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125424ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls random_fields/l-200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c76d0d",
   "metadata": {},
   "source": [
    "Note that the fields have been joined into one (virtual)-raster `random-fields.vrt` using `gdalbuildvrt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a027060",
   "metadata": {},
   "source": [
    "## Estimating damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorrelation_length = 200\n",
    "notebook_id = \"l-{}\".format(decorrelation_length) # for associating files with notebook and notebook-html.\n",
    "\n",
    "# Set the geojson containing OSM-elements and the applied random fields.\n",
    "args = {\n",
    "    \"elements_geojson\" :\"region-assigned.json\",\n",
    "    \"random_fields\" : \"random_fields/l-{}/random_fields.vrt\".format(decorrelation_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587b5b4",
   "metadata": {},
   "source": [
    "NB! If allready processed, results may be reloaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24684f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from file instead\n",
    "segments_gdf = geopandas.read_file(os.path.join(DATADIR,\n",
    "                                                \"run/{}\".format(notebook_id),\n",
    "                                                \"damaged_segments.shp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e082158",
   "metadata": {},
   "source": [
    "## Implementation of the damage function\n",
    "\n",
    "The damage config is estimated and written to file in the notebook `damage-function.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SRCDIR, 'notebooks/damage-func-config.json'), 'r') as infile:\n",
    "    damage_config = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9032f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b51a8e",
   "metadata": {},
   "source": [
    "The $\\varepsilon$ is supposed to be sentered, hence we assume $E[\\varepsilon] = 0$. This should be verified. The `d_sample` value is fitted so as to scale the size of the noise to better agree with the original step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DamageSampler:\n",
    "    # Adapt to current available features.\n",
    "    \n",
    "    def __init__(self, damage_config):\n",
    "        self.beta = np.array([value for key, value in damage_config[\"params\"].items()])\n",
    "        self.eps_std = damage_config[\"eps\"][\"std\"]*damage_config[\"d_sample\"]\n",
    "\n",
    "    def sample(self, depth, velocity, epsilon):\n",
    "        # Implementation of damage function fitted in damage-function.ipynb.\n",
    "        l = np.tile(self.l_hat(self.beta, depth, velocity), (epsilon.shape[0], 1)) * \\\n",
    "            np.exp(self.eps_std*epsilon)\n",
    "        return l / (1 + l)\n",
    "\n",
    "    def l_hat(self, beta, depth, velocity):\n",
    "        return np.abs(beta[0] * depth + beta[1] * velocity + beta[2] * depth * velocity ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a1df2",
   "metadata": {},
   "source": [
    "## Helper function to read raster values.\n",
    "\n",
    "Below are some functions to read directly off the values from the random fields without loading everything into memory. The function is basically taken friom the script `assign_field_from_raster.py`, however it is more convenient to read directly as we estimate the damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dded34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read values of the random field directly from raster.\n",
    "\n",
    "def get_window(rows, cols):\n",
    "        # find window\n",
    "        col_off = min(cols)\n",
    "        row_off = min(rows)\n",
    "        width = max(cols) - min(cols) + 1\n",
    "        height = max(rows) - min(rows) + 1\n",
    "\n",
    "        window_rows = [row - row_off for row in rows]\n",
    "        window_cols = [col - col_off for col in cols]\n",
    "\n",
    "        return Window(col_off, row_off, width, height), window_rows, window_cols\n",
    "\n",
    "def get_raster_values(dataset, feature, rastercoords_from_lonlat):\n",
    "    coords = feature[\"geometry\"][\"coordinates\"]\n",
    "    xs, ys = rastercoords_from_lonlat.transform(*zip(*coords))\n",
    "    rows, cols = rowcol(dataset.transform, xs, ys)\n",
    "    # rows, cols = rowcol(dataset.transform, *zip(*coords))\n",
    "    window, window_rows, window_cols = get_window(rows, cols)\n",
    "    array = dataset.read(out_dtype=np.float64, window=window)\n",
    "    \n",
    "    # It may be problematic to evaluate outside of raster bounds.\n",
    "    try:\n",
    "        return array[:, window_rows, window_cols]\n",
    "\n",
    "    except IndexError as error:\n",
    "        # OSM Segment is outside of raster bounds.\n",
    "        contained_in_raster = [0 <= row < dataset.shape[0] and 0 <= col < dataset.shape[1] for (row, col) in\n",
    "                               zip(rows, cols)]\n",
    "        rows = [row for (contained, row) in zip(contained_in_raster, rows) if contained]\n",
    "        cols = [col for (contained, col) in zip(contained_in_raster, cols) if contained]\n",
    "        window, window_rows, window_cols = get_window(rows, cols)\n",
    "        array = dataset.read(out_dtype=np.float64, window=window)\n",
    "\n",
    "        # append zero values outside of raster bounds.\n",
    "        padded_array = np.zeros([dataset.count, len(contained_in_raster)])\n",
    "        padded_array[:, contained_in_raster] = array[:, window_rows, window_cols]\n",
    "        return padded_array   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9ce67",
   "metadata": {},
   "source": [
    "# Integrate damage function\n",
    "\n",
    "Create feature collection with damaged segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe81272",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"depth\", \"velocity\"]\n",
    "return_periods = [\"020\", \"100\", \"1000\"] # scenarios\n",
    "\n",
    "# feature properties of assigned elements to keep in dataframe.\n",
    "keep_properties = [\"id\", \"highway\", \"bridge\", \"lanes\", \"tunnel\", \"region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args[\"elements_geojson\"], 'r') as file:\n",
    "    flooded_elements = json.load(file)\n",
    "\n",
    "with rasterio.open(args[\"random_fields\"]) as dataset:\n",
    "    damage_assigned = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": [],\n",
    "    }\n",
    "    \n",
    "    rastercoords_from_lonlat = Transformer.from_proj(\n",
    "            Proj('epsg:4326'),  # source coordinates (lonlat)\n",
    "            Proj(dataset.crs),  # target coordinates\n",
    "            always_xy=True  # Use easting-northing, longitude-latitude order of coordinates.\n",
    "        )\n",
    "    \n",
    "    damage_sampler = DamageSampler(damage_config)\n",
    "    \n",
    "    dFi = np.flip(1/np.array(return_periods, dtype=float))  # [0.001, 0.01, 0.05]\n",
    "    counter = 0 # To keep track of how many elements have been filtered.\n",
    "    for element in flooded_elements['features']:\n",
    "        if counter % 100 == 0:\n",
    "            print(\"Elements processed: {}\".format(counter))\n",
    "        epsilon = get_raster_values(dataset, element, rastercoords_from_lonlat)\n",
    "        dx = np.array(element[\"properties\"][\"deltas\"])\n",
    "        \n",
    "        damage_meter = []\n",
    "        for return_period in return_periods:\n",
    "            \n",
    "            # Load flood intensity parameters for each floodmap/return period \n",
    "            # Make sure that selected parameters agrees with raster band names.\n",
    "            depth = np.array(element[\"properties\"][\"spatial_fields\"][\"depth-D312_APA_AI_T{}\".format(return_period)])\n",
    "            velocity = np.array(element[\"properties\"][\"spatial_fields\"][\"velocity-D312_APA_AI_T{}\".format(return_period)])\n",
    "            try:\n",
    "                damage = damage_sampler.sample(depth, velocity, epsilon)\n",
    "            except ValueError as error:\n",
    "                print(\"{} - Check segment: https://www.openstreetmap.org/way/{}\".format(error, element[\"properties\"][\"id\"]))\n",
    "            \n",
    "            # Integration in space\n",
    "            damage_meter.append(np.trapz(damage, dx=dx, axis=1))\n",
    "        damage_arr = np.flip(np.vstack(damage_meter), axis=0)\n",
    "        \n",
    "        # Integration in expectation over return periods\n",
    "        expected_damage_meter = np.trapz(damage_arr, dFi, axis=0)\n",
    "        \n",
    "        # Append filtered to damage_assigned\n",
    "        out_element = {\"type\": \"Feature\", \"geometry\": element[\"geometry\"], \"properties\": {}}\n",
    "        for prop in keep_properties:\n",
    "            out_element[\"properties\"][prop] = element[\"properties\"][prop]\n",
    "        \n",
    "        # Flatten list in order to load as geoDataframe\n",
    "        for index, sample in enumerate(list(np.round(expected_damage_meter, 3))):\n",
    "            out_element[\"properties\"][\"EDM_{}\".format(index)] = sample\n",
    "\n",
    "        out_element[\"properties\"][\"length\"] = np.round(np.sum(dx), 3)\n",
    "        damage_assigned[\"features\"].append(out_element)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38333f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_gdf = geopandas.GeoDataFrame.from_features(damage_assigned, crs=4326)\n",
    "\n",
    "# Remove bridges\n",
    "segments_gdf.drop(segments_gdf[segments_gdf.bridge == \"yes\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e7e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d7bcf",
   "metadata": {},
   "source": [
    "|column|Description\n",
    "|:---|:---\n",
    "|id|Open street map (OSM) id.\n",
    "|highway| Road quality from OSM.\n",
    "|bridge| Whether the segment is a bridge or not.\n",
    "|lanes| Number of lanes.\n",
    "|tunnel| Wheter the segment is a tunnel.\n",
    "|region| region id number loaded from region file (below).\n",
    "|length| Length of segment.\n",
    "|EDM_n | Conditional expected damage meter (sampled).\n",
    "|geometry | Coordinates of segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nr of flooded segments: {}\".format(segments_gdf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548df19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive map of flooded elements.\n",
    "segments_gdf[[\"id\", \"highway\", \"region\", \"length\", \"geometry\"]].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14053551",
   "metadata": {},
   "source": [
    "## Damage for single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = segments_gdf[segments_gdf.id == 132751438]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852cd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "element.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_dm = element.filter(regex = (\"EDM_\\d\")).to_numpy().flatten();\n",
    "print(\"Expected anual damage in meter: {}\".format(expected_dm.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6dd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(expected_dm, bins = 50, density=True);\n",
    "plt.xlabel('EDM');\n",
    "#plt.title(\"EDM for segment {}\".format(element.id.values[0]));\n",
    "plt.savefig(os.path.join(SRCDIR,\"notebooks/figures/edm-segment-{}-{}.png\").format(element.id.values[0], notebook_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497cf0e",
   "metadata": {},
   "source": [
    "Recall that this is the uncertainty in expected damage due to uncertainty in the damage function. That is the lack of knowledge regarding the segments resilience to damage as computed under the given flood scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb2e5da",
   "metadata": {},
   "source": [
    "# Damage estimates for subregions.\n",
    "\n",
    "Next, we aggregate damage according to region. Lets first plot the districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843400f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28961f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = \"nuts/portugal_nuts.shp\"\n",
    "districts_gdf = geopandas.read_file(districts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02bab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge highway tags type and type_link first.\n",
    "highway_type = [h_type for h_type in segments_gdf[\"highway\"].unique() if \"link\" not in h_type]\n",
    "\n",
    "for h_type in highway_type:\n",
    "    segments_gdf.loc[segments_gdf.highway == \"{}_link\".format(h_type), \"highway\"] = h_type\n",
    "\n",
    "segments_gdf[\"highway\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select region by ID_1 (already written contained as column region). \n",
    "# Of course here you could run other regions as well by picking ID_1.\n",
    "# Removing bridges. (Note, it might be a good idea to check validity of this tag).\n",
    "santarem_gdf = segments_gdf[(segments_gdf.region == 16) & (segments_gdf.bridge != \"yes\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5027124",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_cols = [col for col in segments_gdf.columns if \"EDM_\" in col]\n",
    "\n",
    "aggregate_dict = {col: \"sum\" for col in damage_cols}\n",
    "aggregate_dict[\"region\"] = \"count\"\n",
    "aggregate_dict[\"length\"] = \"sum\"\n",
    "\n",
    "santarem_agg_df = santarem_gdf.groupby(\"highway\").agg(aggregate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf086dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of elelements subject to flooding along with their total length (m).\n",
    "santarem_agg_df[[\"region\", \"length\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "santarem_agg_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter so that we are only left with expected damage meter columns.\n",
    "santarem_agg_df = santarem_agg_df.filter(regex = (\"EDM_*\")).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "santarem_agg_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3990035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot damage meter for primary\n",
    "santarem_agg_df.primary.hist(bins=30, density=True);\n",
    "#santarem_agg_df.hist(bins=30, figsize=(15,20), density=True);\n",
    "plt.savefig(os.path.join(SRCDIR,\"notebooks/figures/edm-region-{}-{}.png\").format(\"santarem-primary\", notebook_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8916e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "santarem_agg_df.boxplot()\n",
    "plt.savefig(os.path.join(SRCDIR,\"notebooks/figures/edm-santarem-box-{}.png\").format(notebook_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean damage meter\n",
    "santarem_agg_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8df97",
   "metadata": {},
   "source": [
    "# Whole country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a4dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_agg_df = segments_gdf.groupby(\"highway\").agg(aggregate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da892d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_agg_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove region column\n",
    "portugal_agg_df = portugal_agg_df.filter(regex = (\"EDM_*\")).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee12dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of expected damage meters for entire country. \n",
    "portugal_agg_df.hist(bins=30, figsize=(15,20), density=True);\n",
    "\n",
    "#portugal_agg_df.primary.hist(bins=30, density=True);\n",
    "plt.savefig(os.path.join(SRCDIR,\"notebooks/figures/edm-portugal-hist-{}.png\").format(notebook_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_agg_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61069c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal_agg_df.boxplot()\n",
    "plt.savefig(os.path.join(SRCDIR, \"notebooks/figures/edm-portugal-box-{}.png\").format(notebook_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c40c13",
   "metadata": {},
   "source": [
    "The filtering of osm elements was done to just take into consideration these type of roads (The larger ones). It is easy to add others in the analysis. In particular I see from the map that there are many roads and neighbourhoods that are not taken into the modelling. Take a look at https://wiki.openstreetmap.org/wiki/Key:highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d1fbd",
   "metadata": {},
   "source": [
    "# Mean EDM by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac48652",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_gdf.to_crs(epsg=27429, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419563fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate dammage by region.\n",
    "damage_cols = [col for col in segments_gdf.columns if \"EDM_\" in col]\n",
    "\n",
    "aggregate_dict = {col: \"sum\" for col in damage_cols}\n",
    "aggregate_dict[\"region\"] = \"count\"\n",
    "aggregate_dict[\"length\"] = \"sum\"\n",
    "\n",
    "EDM_by_region_df = segments_gdf.groupby([\"region\", \"highway\"]).agg(aggregate_dict)\n",
    "EDM_by_region_df.rename(columns={\"region\":\"osm_count\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1729b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean over all samples.\n",
    "mean_EDM_by_region_df = EDM_by_region_df.filter(regex=(\"EDM_*\")).mean(axis = 1).unstack(fill_value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_gdf.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66836d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_EDM_gdf = districts_gdf.join(mean_EDM_by_region_df).fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec168fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_EDM_gdf[[\"NUTS_NAME\"] + list(mean_EDM_by_region_df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab159d5a",
   "metadata": {},
   "source": [
    "# Estimates in terms money\n",
    "\n",
    "Road reconstruction costs will depend on a set of different factors. Som factors such as GDP per capita, oil prices may be considered to be nonlocal. Other factors, such as ground and climate conditions may be condiered as local. See the for instance [Developing Cost Estimation Models for Road Rehabilitation and Reconstruction: Case Study of Projects in Europe and Central Asia](https://www.researchgate.net/publication/273616164_Developing_Cost_Estimation_Models_for_Road_Rehabilitation_and_Reconstruction_Case_Study_of_Projects_in_Europe_and_Central_Asia)\n",
    "\n",
    "In the following we create a very simple model based on specifying a mean, a mode and a max. However, it appears unnatural to sample these value independently for each quality of the road. I.e., we expect the prices of different qualities to be dependent. To achieve this, let $F_q$ denote the cummulative distribution of the [triangular distribution](https://en.wikipedia.org/wiki/Triangular_distribution) associated with road of quality $q$. The random cost is then given by \n",
    "$$\n",
    "COST = (F_{motorway}^{-1}(U), ...,F_{trunk}^{-1}(U))\n",
    "$$\n",
    "where $U$ is a uniform random variable on $[0,1]$.\n",
    "\n",
    "Note that the input numbers mean, mode and max applied refers to the average reconstruction cost predicted for the future year for portugal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price estimates in M€/m (million euro per meter)\n",
    "os.chdir(SRCDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b51644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import COST_ROAD\n",
    "pd.DataFrame.from_dict(COST_ROAD, orient=\"index\", columns=[\"min\", \"mode\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40831c8f",
   "metadata": {},
   "source": [
    " Price range used on cost for different road qualities in millions of euro per meter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the inverse cummulative distribution.\n",
    "from math import sqrt\n",
    "\n",
    "def F_q_inv(a, c, b):\n",
    "    F = (c-a)/(b-a)\n",
    "    #return lambda z: 0.5*(a + c)*z*z + 0.5*(c-a)*z + b\n",
    "    return lambda U: a + sqrt(U*(b-a)*(c-a)) if U < F  else b - sqrt((1-U)*(b-a)*(b-c))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dict = {key:F_q_inv(*value) for (key,value) in COST_ROAD.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.random.uniform(0,1,10000)\n",
    "cost_motorway = np.array([f_dict['motorway'](xi) for xi in U])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addc684",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cost_motorway, bins=60, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f398b",
   "metadata": {},
   "source": [
    "Sampling cost using a triangular distribution for motorways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a24988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling cost.\n",
    "samples = 5000\n",
    "\n",
    "U = np.random.uniform(0,1,samples)\n",
    "cost_columns = [\"COST_{}\".format(sample) for sample in range(samples)] \n",
    "#cost_df = pd.DataFrame(np.vstack([pert(*COST_ROAD[key],samples) for key in COST_ROAD.keys()]), \n",
    "#                       index= COST_ROAD.keys(), \n",
    "#                       columns=cost_columns)\n",
    "cost_df = pd.DataFrame(np.vstack([np.array([f_dict[key](xi) for xi in U]) for key in COST_ROAD.keys()]), \n",
    "                       index= COST_ROAD.keys(), \n",
    "                       columns=cost_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3433aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(cost_df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ccaf9",
   "metadata": {},
   "source": [
    "Pairs plot of the sampled cost range for different qualities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDM_by_region = EDM_by_region_df.filter(regex=(\"EDM_*\")).unstack(fill_value=0.)\n",
    "\n",
    "# Multiply cost times damage.\n",
    "COST_by_region = pd.concat([EDM_by_region[d_sample].dot(cost_df) for d_sample in damage_cols],\n",
    "                           axis=1,\n",
    "                           keys=damage_cols,\n",
    "                           names=[\"spatial_samples\", \"cost_samples\"])\n",
    "\n",
    "# Get names dictionary.\n",
    "region_names = districts_gdf[\"NUTS_NAME\"].to_dict()\n",
    "COST_by_region.index.to_series().map(region_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521686f1",
   "metadata": {},
   "source": [
    "Multiplying cost per meter with expected annual dammage meter yield the expected annual cost (EAC). This is done pointwise for each sample yielding a distriubution of EAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c33d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COST_by_region.sum().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "COST_by_region.sum().hist(figsize=(10, 8), bins=40, density=True);\n",
    "plt.savefig(\"notebooks/figures/eac-portugal-{}.png\".format(notebook_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf5e09a",
   "metadata": {},
   "source": [
    "Density of total expected annual cost for the entire country in Millions of Euro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_COST_by_region = districts_gdf.join(pd.concat([COST_by_region.mean(axis=1), COST_by_region.std(axis=1)], axis=1, keys=[\"mean\", \"std\"]),).fillna(0.)\n",
    "mean_COST_by_region[[\"NUTS_NAME\", \"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_COST_by_region[mean_COST_by_region.EAC != 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as cx\n",
    "\n",
    "# Add region coordinate for tags\n",
    "districts_gdf['coords'] = districts_gdf['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "districts_gdf['coords'] = [coords[0] for coords in districts_gdf['coords']]\n",
    "\n",
    "mean_COST_by_region = districts_gdf.join(pd.Series(COST_by_region.mean(axis=1), name=\"EAC\")).fillna(0.)\n",
    "# Skip regions with zero EAC.\n",
    "mean_COST_by_region = mean_COST_by_region[mean_COST_by_region.EAC != 0.]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 15))\n",
    "\n",
    "mean_COST_by_region.plot(ax=ax, column=\"EAC\", legend=True, alpha=0.8, edgecolor='k')\n",
    "\n",
    "for idx, row in mean_COST_by_region.iterrows():\n",
    "    plt.annotate(row['NUTS_NAME'], xy=row['coords'], horizontalalignment='center')\n",
    "\n",
    "cx.add_basemap(ax,source=cx.providers.OpenStreetMap.Mapnik, crs=mean_COST_by_region.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4fcd7",
   "metadata": {},
   "source": [
    "# Save run to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATADIR)\n",
    "out_folder = \"run/{}\".format(notebook_id)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AEDM (Average Expexted Damage Meter per year)\n",
    "segments_gdf[\"AEDM\"] = segments_gdf.filter(regex = (\"EDM_*\")).mean(axis=1)\n",
    "\n",
    "# Compute AEDM (Average Expexted Damage Ratio) \n",
    "segments_gdf[\"AEDR%\"] = 100*segments_gdf[\"AEDM\"]/segments_gdf[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477815f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write geodataframe to file.\n",
    "outfile = os.path.join(out_folder, \"damaged_segments.shp\")\n",
    "#cols = [\"id\",\"highway\",\"lanes\",\"tunnel\",\"region\",\"length\",\"AEDM\",\"AEDR%\", \"geometry\"]\n",
    "segments_gdf.to_file(filename=outfile, driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3134099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to files for plotting.\n",
    "santarem_agg_df.to_csv(os.path.join(out_folder, \"santarem_agg.csv\"))\n",
    "portugal_agg_df.to_csv(os.path.join(out_folder, \"portugal_agg.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48406972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp-damage-aggregator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
