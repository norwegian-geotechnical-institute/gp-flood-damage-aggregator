{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8824a72",
   "metadata": {},
   "source": [
    "# Fitting the damage function.\n",
    "\n",
    "The purpose of this notebook is to create a continuous damage function based on the table specifying damage for different threshold classes. The output are a parameter file and some figures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "DATADIR = \"/home/erlend/data/portugal-rerun\"\n",
    "SRCDIR = \"/home/erlend/github/gp-flood-damage-aggregator\"\n",
    "# Set environment variable DATADIR\n",
    "%env DATADIR $DATADIR\n",
    "\n",
    "#Local import (change working directory), set to project src dir.\n",
    "os.chdir(SRCDIR)\n",
    "os.getcwd()\n",
    "\n",
    "from betapert import pert, plot_damage, check_stats\n",
    "from config import DAMAGE_ROAD_STATIC, DAMAGE_ROAD_DYNAMIC\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIOS = [\"D312_APA_AI_T{}\".format(ret) for ret in [\"020\", \"100\", \"1000\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open features.tif for each scenario to get an overview of the raster values.\n",
    "feature_file = os.path.join(DATADIR, \"floodmaps/merged_floodmaps\", \"features.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b431b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(feature_file) as dataset:\n",
    "    print(\"Name: {}\".format(dataset.name))\n",
    "    depth = dataset.read(5).flatten()\n",
    "    print(\"depth: {}\".format(dataset.descriptions[4]))\n",
    "    velocity = dataset.read(6).flatten()\n",
    "    print(\"veloicty: {}\".format(dataset.descriptions[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fc250",
   "metadata": {},
   "source": [
    "Let us only consider depth in the range 0 to 6 meters. The dataset also contains fare larger depths, however these are usually where there are already rivers when there is no flooding, and hence not relevant for damage assessment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eea425",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_not_0 = np.logical_and(depth != 0, depth < 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e014db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of values.\n",
    "plt.hist(depth[depth_not_0], bins = 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12839bba",
   "metadata": {},
   "source": [
    "There is certainly too many samples at high depth range. One option is to rather sample values according to some chosen distribution. For now, we will keep it like this, assuming it does not have too large of an impact on the fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e135821",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(depth_not_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd501b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(velocity[depth_not_0], log=True, bins = 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea707f36",
   "metadata": {},
   "source": [
    "## Correlation of velocity and depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82324790",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.scatter(depth[depth_not_0][:50000], velocity[depth_not_0][:50000], alpha=0.1)\n",
    "ax.set_xlabel(\"Depth\")\n",
    "ax.set_ylabel(\"Velocity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9db33",
   "metadata": {},
   "source": [
    "## Fitting a suitable damage function.\n",
    "\n",
    "The damage information is described in terms of a number from 0 to 1. The damage function relates depth, or depth and velocity to damage. The available information is given as three categories in terms of depth and two categories in terms of velocity. Cat1 (d<0.5), Cat2 (0.5 < d < 2.0), Cat3 (2<d) where d is depth. Further velocity is split in lower or higher than 1. Values of damage for each cathegory are expressed in a range of low, mean, high.\n",
    "\n",
    "This provides means of sampling damage values as a function of depth and velocity. However, as seen below values are highly dependent on cathegory, in particular the velocity is highly discontinuous. Consequently a relatively simple model might reasonably well capture values, without the artificial discontinuity. Further it may provide more efficient sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_damage(DAMAGE_ROAD_DYNAMIC, samples=500000, bins=100, file_name=\"notebooks/figures/beta_dist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a nicer plot of the distributions for article.\n",
    "\n",
    "import seaborn as sns\n",
    "from betapert import pert\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "data = []\n",
    "for nr, (key, (a, b, c)) in enumerate(DAMAGE_ROAD_DYNAMIC.items()):\n",
    "    data.extend([(key, v) for v in pert(a, b, c, 500000)])\n",
    "\n",
    "sampled_damage_df = pd.DataFrame(data, columns=[\"Category\", \"Damage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.displot(\n",
    "    sampled_damage_df, \n",
    "    x=\"Damage\", \n",
    "    hue=\"Category\", \n",
    "    kind=\"kde\", \n",
    "    fill=True, \n",
    "    bw_adjust= 1.5, \n",
    "    #aspect=1.8, \n",
    "    gridsize=200, \n",
    "    legend=False\n",
    ")\n",
    "ax.set(xlim=(0, 1.))\n",
    "plt.legend(labels=[\"$2 < h$\",\"$0.5 < h < 2$\", \"$h < 0.5$\"])\n",
    "plt.savefig(\"notebooks/figures/sns_beta_dist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1320e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAMAGE_ROAD_DYNAMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To sample, simply call\n",
    "pert(*DAMAGE_ROAD_DYNAMIC['Cat2'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4edd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(DAMAGE_ROAD_STATIC, index=[\"minimum\", \"mode\", \"maximum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22604b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(DAMAGE_ROAD_DYNAMIC, index=[\"minimum\", \"mode\", \"maximum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pert_parameters(depth, velocity):\n",
    "    cat_nr = len([x for x in [0.5,2] if x < depth]) + 1\n",
    "    if velocity < 1:\n",
    "        return DAMAGE_ROAD_STATIC[\"Cat{}\".format(cat_nr)]\n",
    "    else:\n",
    "        return DAMAGE_ROAD_DYNAMIC[\"Cat{}\".format(cat_nr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({\"depth\" : np.random.uniform(low=0, high=5, size=1000),\n",
    "#                            \"velocity\": np.random.uniform(low=0, high=2, size=1000)})\n",
    "number_of_samples = 500000\n",
    "df = pd.DataFrame({\"depth\" : depth[depth_not_0][:number_of_samples], \n",
    "                   \"velocity\": velocity[depth_not_0][:number_of_samples]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4086f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46019c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"damage\"] = df.apply(lambda row: float(pert(*get_pert_parameters(row[\"depth\"], row[\"velocity\"]),1)[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df[:10000].plot.scatter(\"depth\", \"damage\", alpha=0.3).get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c74fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"notebooks/figures/depth-damage-treshold.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10000].plot.scatter(\"velocity\", \"damage\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "samples = 50000\n",
    "ax.scatter(df[:samples][\"depth\"], df[:samples][\"velocity\"], df[:samples][\"damage\"], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f15de3",
   "metadata": {},
   "source": [
    "It is clear that the threshold version is not ideal. Let us find a suitable substitute and fit the parameters. The recovered function will have to satisfy:\n",
    "\n",
    "- Bounded in [0,1].\n",
    "- monotone in both parameters.\n",
    "- damage=0 for height=0 and velocity=0. \n",
    "\n",
    "One option is to transform the target according to \n",
    "$$\n",
    "    d = \\frac{L}{1 + L} \\quad \\Leftrightarrow \\quad L = \\frac{d}{1-d}.\n",
    "$$\n",
    "Then create a model so that $L = \\hat L\\exp(\\varepsilon)$ and $\\hat L$ is a model for $L$ based on a linear combination of $h, v$ and also possibly nonlinear terms $hv$ and $v^2$ as parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2055b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"l\"] = df.apply(lambda row: row[\"damage\"]/(1-row[\"damage\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible features.\n",
    "df[\"velocity_sq\"] = df.apply(lambda row: row[\"velocity\"]**2, axis=1)\n",
    "df[\"moment\"] = df.apply(lambda row: row[\"velocity\"]*row[\"depth\"], axis=1)\n",
    "df[\"const\"] = df.apply(lambda row: 1., axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff13c80",
   "metadata": {},
   "source": [
    "As we can tell from the above plot, $\\varepsilon$ is close to normal, however as $\\varepsilon$ is not centered $\\hat L$ should be scaled in order to obtain a better fit for the damage values. That is, applying least square on the level of transformed values $L$ does not translate to an optimal fit for the damage values. This requires a nonlilnear optimization procedure. To this end note that \n",
    "$$\n",
    "\\varepsilon = \\log(L/\\hat L)\n",
    "$$\n",
    "Hence, a natural loss function $\\sum \\varepsilon_i^2 = \\sum \\log(L_i/\\hat L_i)^2$. (Compare with cross entropy loss.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c258cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_hat(beta, depth, velocity):\n",
    "    return np.abs(beta[0]*depth + beta[1]*velocity + beta[2]*depth*velocity**2)\n",
    "    #return beta[0]*depth + beta[1]*velocity**2\n",
    "\n",
    "def d_hat(beta, depth, velocity):\n",
    "    l_preds = l_hat(beta, depth, velocity)\n",
    "    return l_preds/(1 + l_preds)\n",
    "\n",
    "def eps(beta, depth, velocity, l):\n",
    "    return np.log(l) - np.log(l_hat(beta, depth, velocity))  \n",
    "                              \n",
    "def loss(beta):\n",
    "    return np.mean(np.square(eps(beta, df[\"depth\"], df[\"velocity\"], df[\"l\"])))\n",
    "\n",
    "beta_0 = np.array([.01, .01, .01])\n",
    "res = minimize(loss, beta_0, method='Nelder-Mead', tol=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173cbf0a",
   "metadata": {},
   "source": [
    "Testing with different variables:\n",
    "\n",
    "| d | dv | v | v^2 | dv^2 |loss |\n",
    "|---|----|---|-----|------|-----|\n",
    "| x | x  | x | x   |      |0.99 |\n",
    "| x | x  | x |     |      |0.99 |\n",
    "| x |    |   | x   |      |1.12 |\n",
    "| x | x  |   |     |      |1.18 |\n",
    "| x | x  |   | x   |      |1.11 |\n",
    "| x |    | x |     |      |1.28 |\n",
    "| x |    | x |     |  x   |0.97 |\n",
    "\n",
    "It seems that a model usiong depth, velocity and kinetic energy is suitable. Hence, the prediction model looks like\n",
    "$$\n",
    "\\hat d = \\frac{\\hat L}{1 + \\hat L} \\mbox{ where } \\hat L = abs(\\beta ^T x)\n",
    "$$\n",
    "where beta is the coefficients and $x = (d,v,dv^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8e85b",
   "metadata": {},
   "source": [
    "Let us consider the new residuals using the model derived from the nonlinear optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"l_hat\"] = l_hat(res.x, df[\"depth\"], df[\"velocity\"])\n",
    "df[\"d_hat\"] = d_hat(res.x, df[\"depth\"], df[\"velocity\"])\n",
    "df[\"d_residuals\"] = df[\"d_hat\"] - df[\"damage\"]\n",
    "df[\"eps\"] = eps(res.x, df[\"depth\"], df[\"velocity\"], df[\"l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make nicer figure for article.\n",
    "\n",
    "sns.set(font_scale=1.8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.histplot(df['eps'], kde=False, stat='density', bins=80)\n",
    "ax.set(xlim=(-4.5, 4.5))\n",
    "ax.set(xlabel='Residuals', ylabel='Density')\n",
    "\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x_pdf = np.linspace(xmin, xmax, 100)\n",
    "y_pdf = norm.pdf(x_pdf, df[\"eps\"].mean(), df[\"eps\"].std())\n",
    "sns.lineplot(x=x_pdf,y=y_pdf, lw=2)\n",
    "\n",
    "plt.savefig(\"notebooks/figures/sns-eps-density.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1db13",
   "metadata": {},
   "source": [
    "It appears to be quite close to normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"d_residuals\"], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"damage residuals mean: {}, std:{}\".format(df[\"d_residuals\"].mean(), df[\"d_residuals\"].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefe423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[0:20000].plot(kind=\"scatter\", x=\"damage\", y=\"eps\", alpha=0.1, \n",
    "                 xlabel=\"predicted damage\", ylabel=\"residuals\", logx=True)\n",
    "#plt.scatter(x=df[:20000][\"d_hat\"], y=df[:20000][\"eps\"], alpha=0.1)\n",
    "plt.savefig(\"notebooks/figures/d_hat-eps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abea496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:20000].plot(kind=\"scatter\", x=\"d_hat\", y=\"damage\", alpha=0.1, \n",
    "                 xlabel=\"predicted damage\", ylabel=\"damage\", logx=True, logy=True)\n",
    "#plt.scatter(x=df[:20000][\"d_hat\"], y=df[:20000][\"eps\"], alpha=0.1)\n",
    "plt.savefig(\"notebooks/figures/d_hat-damage.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f47a0f",
   "metadata": {},
   "source": [
    "Recall that $\\varepsilon$ is scaled with $l$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicer figure, for publication.\n",
    "\n",
    "nr_of_samples=15000\n",
    "sns.set(font_scale=1.3)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "threshold_size = 15\n",
    "threshold_alpha = 0.5\n",
    "blue = sns.color_palette()[0]\n",
    "orange = sns.color_palette()[1]\n",
    "\n",
    "\n",
    "df[0:nr_of_samples].plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"depth\", \n",
    "    y=\"damage\", \n",
    "    color=orange, \n",
    "    alpha=threshold_alpha, \n",
    "    ax=axs[0], \n",
    "    label=\"thresholds\",\n",
    "    marker='s',\n",
    "    s=threshold_size,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.4\n",
    ")\n",
    "df[0:nr_of_samples].plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"depth\", \n",
    "    y=\"d_hat\", \n",
    "    color=blue, \n",
    "    alpha=0.4, \n",
    "    label=\"fitted\", \n",
    "    ax=axs[0],\n",
    "    s=10,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.3\n",
    ")\n",
    "\n",
    "df[0:nr_of_samples].plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"velocity\", \n",
    "    y=\"damage\", \n",
    "    color=orange, \n",
    "    alpha=threshold_alpha, \n",
    "    ax=axs[1], \n",
    "    s=threshold_size,\n",
    "    marker='s',\n",
    "    label=\"thresholds\",\n",
    "    edgecolors='black',\n",
    "    linewidth=0.3\n",
    ")\n",
    "df[0:nr_of_samples].plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"velocity\", \n",
    "    y=\"d_hat\", \n",
    "    color=blue, \n",
    "    alpha=0.4, \n",
    "    label=\"fitted\", \n",
    "    s=10,\n",
    "    ax=axs[1],\n",
    "    edgecolors='black',\n",
    "    linewidth=0.4\n",
    ")\n",
    "\n",
    "axs[0].set_xlabel(\"Depth [m]\")\n",
    "axs[0].set_ylabel(\"Damage\")\n",
    "axs[1].set_xlabel(\"Velocity [m/s]\")\n",
    "for ax in axs:\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_ylim(1e-4,1)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "fig.legend(\n",
    "    labels=[\"thresholds\", \"fitted\"], \n",
    "    loc=\"upper center\", \n",
    "    bbox_to_anchor=(0.99, 0.87),\n",
    "    fancybox=False, \n",
    "    shadow=False, \n",
    "    ncol=1)\n",
    "\n",
    "fig.savefig(\"notebooks/figures/fitted-thresholds-scatter-vel-depth.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292b38f",
   "metadata": {},
   "source": [
    "## sample damage\n",
    "\n",
    "Lets sample values according to $L = \\hat L \\exp(\\varepsilon)$. If we sample $\\varepsilon$ according to the observed residuals we obtain a problem. The uncertainty generated by the fitted model will be larger than the one indicated in the table. This is due to the large discontinuities in the sampled model (which are the ones we want to get rid of). As a remedy one might like to scale $\\varepsilon$ so as to obtain values that agrees well with the original bound in the damage table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96daf911",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_mean, eps_std = df[\"eps\"].mean(), df[\"eps\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_damage(depth, velocity, scale_factor):\n",
    "    xi = np.random.normal(scale=scale_factor*eps_std, size=depth.size)\n",
    "    l = l_hat(res.x, depth, velocity)*np.exp(xi)\n",
    "    return l/(1+l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = [0.3,0.4,0.5,1.0]\n",
    "for scale_factor in scale_factors:\n",
    "    df[\"d_sample_{}\".format(scale_factor)] = sample_damage(df[\"depth\"], df[\"velocity\"], scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.3)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "nr_of_samples=15000\n",
    "d_sample = \"d_sample_1.0\"\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "threshold_size = 15\n",
    "threshold_alpha = 0.5\n",
    "blue = sns.color_palette()[0]\n",
    "orange = sns.color_palette()[1]\n",
    "\n",
    "\n",
    "df[0:nr_of_samples].plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"depth\", \n",
    "    y=\"damage\", \n",
    "    color=orange, \n",
    "    alpha=threshold_alpha, \n",
    "    ax=axs[0], \n",
    "    label=\"thresholds\",\n",
    "    marker='s',\n",
    "    s=threshold_size,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.4\n",
    ")\n",
    "df[0:nr_of_samples].plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"depth\", \n",
    "    y=d_sample, \n",
    "    color=blue, \n",
    "    alpha=0.4, \n",
    "    label=\"sampled\", \n",
    "    ax=axs[0],\n",
    "    s=10,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.3\n",
    ")\n",
    "\n",
    "\n",
    "df[0:nr_of_samples].plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"velocity\", \n",
    "    y=\"damage\", \n",
    "    color=orange, \n",
    "    alpha=threshold_alpha, \n",
    "    ax=axs[1], \n",
    "    s=threshold_size,\n",
    "    marker='s',\n",
    "    label=\"thresholds\",\n",
    "    edgecolors='black',\n",
    "    linewidth=0.3\n",
    ")\n",
    "df[0:nr_of_samples].plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"velocity\", \n",
    "    y=d_sample, \n",
    "    color=blue, \n",
    "    alpha=0.4, \n",
    "    label=\"sampled\", \n",
    "    s=10,\n",
    "    ax=axs[1],\n",
    "    edgecolors='black',\n",
    "    linewidth=0.4\n",
    ")\n",
    "\n",
    "axs[0].set_xlabel(\"Depth [m]\")\n",
    "axs[0].set_ylabel(\"Damage\")\n",
    "axs[1].set_xlabel(\"Velocity [m/s]\")\n",
    "for ax in axs:\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_ylim(1e-4,1)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "fig.legend(\n",
    "    labels=[\"thresholds\", \"sampled\"], \n",
    "    loc=\"upper center\", \n",
    "    bbox_to_anchor=(0.99, 0.87),\n",
    "    fancybox=False,\n",
    "    shadow=False, \n",
    "    ncol=1)\n",
    "\n",
    "fig.savefig(\"notebooks/figures/sampled-thresholds-scatter-vel-depth-{}.png\".format(d_sample), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative figure.\n",
    "\n",
    "import seaborn.objects as so\n",
    "import matplotlib.colors as colors\n",
    "#sns.set(font_scale=1.8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plot_data_df = df[0:100000]\n",
    "fig, axs = plt.subplots(1, 3, sharey=True, sharex=True)\n",
    "#p1 = so.Plot(plot_data_df, \"velocity\", \"depth\", ax=ax) \n",
    "#p1.add(so.Dots(alpha=0.8), color = \"damage\")\n",
    "norm = colors.LogNorm(1e-3,1)\n",
    "\n",
    "p1 = sns.scatterplot(\n",
    "    data=plot_data_df, \n",
    "    y=\"velocity\", \n",
    "    x=\"depth\", \n",
    "    hue=\"damage\",\n",
    "    hue_norm = norm,\n",
    "    ax=axs[0],\n",
    "    \n",
    ")\n",
    "\n",
    "p2 = sns.scatterplot(\n",
    "    data=plot_data_df, \n",
    "    y=\"velocity\", \n",
    "    x=\"depth\", \n",
    "    hue=\"d_hat\", \n",
    "    hue_norm = norm,\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "p2 = sns.scatterplot(\n",
    "    data=plot_data_df, \n",
    "    y=\"velocity\", \n",
    "    x=\"depth\", \n",
    "    hue=\"d_sample_1.0\",\n",
    "    hue_norm = norm,\n",
    "    ax=axs[2]\n",
    ")\n",
    "# Remove legends\n",
    "for ax in axs:\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim(0,6)\n",
    "    ax.set_ylim(1e-2,6)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "# Add colorbar\n",
    "\n",
    "#Normalize(0,1)\n",
    "cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "#Add colorbar\n",
    "left = axs[2].get_position().x1+0.05\n",
    "bottom = axs[2].get_position().y0\n",
    "width = 0.05\n",
    "height = axs[2].get_position().height\n",
    "\n",
    "cax = fig.add_axes([left,bottom,width,height])\n",
    "fig.colorbar(sm, cax=cax, label=\"Damage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece8699",
   "metadata": {},
   "source": [
    "## Recreating damage table from sampled values.\n",
    "\n",
    "Is it possible to move in opposite direction, i.e. find estimates on the dynamic and static flooding for different ranges of depths using the fitted damage function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25925ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"isCat3\"] = (df[\"depth\"] > 2)\n",
    "df[\"isCat2\"] = (df[\"depth\"] < 2) & (df[\"depth\"] > 0.5)\n",
    "df[\"isCat1\"] = (df[\"depth\"] < 0.5)\n",
    "df[\"isDyn\"] = (df[\"velocity\"] > 1.)\n",
    "df[\"isStat\"] = (df[\"velocity\"] < 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94d4c1",
   "metadata": {},
   "source": [
    "Lets first look at how the predicted values are distributed according to cathegory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "cat_stats_d_hat_df = pd.DataFrame(columns = ['cat', 'd', 'cont_mean', 'tresh_mean', 'samples'])\n",
    "d_sample = \"d_sample_1.0\"\n",
    "\n",
    "fig, axs = plt.subplots(6, figsize=(10,25))\n",
    "i = 0\n",
    "for cat,d in product([\"isCat3\",\"isCat2\",\"isCat1\"], [\"isDyn\", \"isStat\"]):\n",
    "    subset = df[cat] & df[d]\n",
    "    bins = np.linspace(0, max(df[subset][\"damage\"].max(), df[subset][d_sample].max()), 50)\n",
    "    axs[i].hist(df[subset][\"damage\"], bins, alpha=0.5, label='thresholds')\n",
    "    axs[i].hist(df[subset][\"d_hat\"], bins, alpha=0.5, label='continuous')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(\"depth: {}, velocity: {}\".format(\n",
    "        cat.replace(\"is\",\"\"),\n",
    "        d.replace(\"is\",\"\")\n",
    "    ))\n",
    "    cat_stats_d_hat_df = cat_stats_d_hat_df.append({'cat':cat, 'd': d, 'cont_mean': df[subset][\"d_hat\"].mean(), \n",
    "                         'tresh_mean': df[subset][\"damage\"].mean(), 'samples':df[subset].shape[0]}, ignore_index = True)\n",
    "    i = i+1\n",
    "fig.savefig(\"notebooks/figures/damage-table-hist-d_hat.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e7ab8",
   "metadata": {},
   "source": [
    "As one would expect, continuous predictions appears more smeared out in these cathgories. However, one should keep in mind that we have been picking cathegories conforming to the ones used for sampling. The continuous model is supposed to fit also those not conforming to the ones we have sampled from. Let us see what happens when we add noise to predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_stats_s_df = pd.DataFrame(columns = ['cat', 'd', 'cont_mean', 'tresh_mean', 'samples'])\n",
    "d_sample = \"d_sample_1.0\"\n",
    "\n",
    "fig, axs = plt.subplots(6, figsize=(10,25))\n",
    "i = 0\n",
    "for cat,d in product([\"isCat3\",\"isCat2\",\"isCat1\"], [\"isDyn\", \"isStat\"]):\n",
    "    subset = df[cat] & df[d]\n",
    "    bins = np.linspace(0, max(df[subset][\"damage\"].max(), df[subset][d_sample].max()), 50)\n",
    "    axs[i].hist(df[subset][\"damage\"], bins, alpha=0.5, label='thresholds')\n",
    "    axs[i].hist(df[subset][d_sample], bins, alpha=0.5, label='continuous')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(\"depth: {}, velocity: {}\".format(\n",
    "        cat.replace(\"is\",\"\"),\n",
    "        d.replace(\"is\",\"\")\n",
    "    ))\n",
    "    cat_stats_s_df = cat_stats_s_df.append({'cat':cat, 'd': d, 'cont_mean': df[subset][d_sample].mean(), \n",
    "                         'tresh_mean': df[subset][\"damage\"].mean(), 'samples':df[subset].shape[0]}, ignore_index = True)\n",
    "    i = i+1\n",
    "fig.savefig(\"notebooks/figures/damage-table-hist-{}.png\".format(d_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_stats_s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_stats_d_hat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f9e841",
   "metadata": {},
   "source": [
    "## Write fitted values to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395da13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(d_sample[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"date\": str(date.today()),\n",
    "    \"source\": \"damage-function.ipynb\",\n",
    "    \"params\": {\"depth\": res.x[0], \"velocity\": res.x[1], \"depth_velocity_2\": res.x[2]},\n",
    "    \"eps\": {\"mean\": eps_mean, \"std\": eps_std},\n",
    "    \"d_sample\": float(d_sample[-3:])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('notebooks/damage-func-config.json', 'w') as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36129ebc",
   "metadata": {},
   "source": [
    "## Unknown velocity.\n",
    "\n",
    "In many cases velocity is unknown, and one needs a damage function that only depends on depth. Perhaps the most obvious way is to refit a function, only depending on depth, while still using the sampled values. This enables us to mix the two tables for dynamic and static flooding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_hat(beta, depth):\n",
    "    return np.abs(beta[0]*depth + beta[1]*depth**2)\n",
    "    #return beta[0]*depth + beta[1]*velocity**2\n",
    "\n",
    "def d_hat(beta, depth):\n",
    "    l_preds = l_hat(beta, depth)\n",
    "    return l_preds/(1 + l_preds)\n",
    "\n",
    "def eps(beta, depth, l):\n",
    "    return np.log(l) - np.log(l_hat(beta, depth))  \n",
    "                              \n",
    "def loss(beta):\n",
    "    return np.mean(np.square(eps(beta, df[\"depth\"], df[\"l\"])))\n",
    "\n",
    "beta_0 = np.array([.01, .01, .01])\n",
    "\n",
    "#Refit the values!\n",
    "res = minimize(loss, beta_0, method='Nelder-Mead', tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d14122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new predictions\n",
    "df[\"l_hat\"] = l_hat(res.x, df[\"depth\"])\n",
    "df[\"d_hat\"] = d_hat(res.x, df[\"depth\"])\n",
    "df[\"d_residuals\"] = df[\"d_hat\"] - df[\"damage\"]\n",
    "df[\"eps\"] = eps(res.x, df[\"depth\"], df[\"l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram.\n",
    "plt.hist(df[\"eps\"], density=True, bins=60)\n",
    "\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "plt.plot(x, norm.pdf(x, df[\"eps\"].mean(), df[\"eps\"].std()))\n",
    "plt.savefig(\"notebooks/figures/depth-eps-density.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35567a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_mean = df[\"xi\"].mean()\n",
    "eps_std = df[\"xi\"].std()\n",
    "print(\"eps mean: {}, eps std:{}\".format(xi_mean, xi_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"d_residuals\"], bins=40);\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5213634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"damage residuals mean: {}, std:{}\".format(df[\"d_residuals\"].mean(), df[\"d_residuals\"].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790aa17",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00081e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_damage(depth, scale_factor):\n",
    "    xi = np.random.normal(scale=scale_factor*eps_std, size=depth.size)\n",
    "    l = l_hat(res.x, depth)*np.exp(xi)\n",
    "    return l/(1+l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = [0.3,0.4,0.5,1.0]\n",
    "for scale_factor in scale_factors:\n",
    "    df[\"d_sample_{}\".format(scale_factor)] = sample_damage(df[\"depth\"], scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce438a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sample = \"d_sample_1.0\"\n",
    "ax = df[:10000].plot(kind=\"scatter\", x=\"depth\", y=d_sample, color=\"blue\", alpha=0.3, label=\"sampled\")\n",
    "fig = df[:10000].plot(kind=\"scatter\", x=\"depth\", y=\"damage\", color=\"orange\", alpha=0.3, ax=ax, label=\"thresholds\").get_figure()\n",
    "fig.savefig(\"notebooks/figures/depth-sampled-thresholds-scatter-depth-{}.png\".format(d_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be839b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[:10000].plot(kind=\"scatter\", x=\"velocity\", y=d_sample, color=\"blue\", alpha=0.3, label=\"sampled\")\n",
    "fig = df[:10000].plot(kind=\"scatter\", x=\"velocity\", y=\"damage\", color=\"orange\", alpha=0.3, ax=ax, label=\"thresholds\").get_figure()\n",
    "fig.savefig(\"notebooks/figures/depth-sampled-thresholds-scatter-velocity-{}.png\".format(d_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1fd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "fig, axs = plt.subplots(3, figsize=(10,15))\n",
    "i = 0\n",
    "for cat in [\"isCat3\",\"isCat2\",\"isCat1\"]:\n",
    "    subset = df[cat]\n",
    "    bins = np.linspace(0, max(df[subset][\"damage\"].max(), df[subset][d_sample].max()), 50)\n",
    "    axs[i].hist(df[subset][\"damage\"], bins, alpha=0.5, label='thresholds')\n",
    "    axs[i].hist(df[subset][d_sample], bins, alpha=0.5, label='continuous')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(\"depth: {}\".format(cat.replace(\"is\",\"\")))\n",
    "    i = i+1\n",
    "    \n",
    "fig.savefig(\"notebooks/figures/depth-damage-table-hist-{}.png\".format(d_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e6d46",
   "metadata": {},
   "source": [
    "## Write fitted values to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc32416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e41d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(d_sample[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10814ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sample[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"date\": str(date.today()),\n",
    "    \"source\": \"damage-function.ipynb\", \n",
    "    \"params\": {\"depth\": res.x[0], \"depth_2\": res.x[1]},\n",
    "    \"epsilon\": {\"mean\": eps_mean, \"std\": eps_std},\n",
    "    \"d_sample\": float(d_sample[-3:]),\n",
    "    \"gamma\": gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('depth-damage-func-config.json', 'w') as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e310fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp-damage-aggregator",
   "language": "python",
   "name": "gp-damage-aggregator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
